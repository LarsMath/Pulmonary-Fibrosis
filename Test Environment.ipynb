{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tqdm import tqdm\nimport seaborn as sns\nimport wandb\nfrom wandb.keras import WandbCallback\nimport keras\nfrom keras.models import Sequential\n\nfrom pfutils import (get_test_data, get_train_data, get_pseudo_test_data,\n                     build_model, get_cosine_annealing_lr_callback, get_fold_indices, DataGenerator)\n\nWANDB = True\nSUBMIT = False\nDATA_GENERATOR = True\nTRAIN_ON_BACKWARD_WEEKS = False\n\n#If TEST is False use this to simulate tractable testcases. Should be 0 if SUBMIT = True\nPSEUDO_TEST_PATIENTS = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    PSEUDO_TEST_PATIENTS = 0\n    WANDB = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if WANDB:    \n    # retrieve W&B key\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb_key\")\n    assert wandb_key, \"Please create a key.txt or Kaggle Secret with your W&B API key\"\n\n    #wandb_key = \"24020b558f39257d30a084a55cb438922c321495\"\n\n    !pip install -q --upgrade wandb\n    !wandb login $wandb_key","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Settings And network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of folds. A number between 1 and 176-PSEUDO_TEST_PATIENTS\nFOLDS = 10\n\n#Batch size\nBATCH_SIZE = 128\n\n#Amount of features inputted in NN\nNUMBER_FEATURES = 9\n\n#Hidden layers\nHIDDEN_LAYERS = [64,64]\n\n#State whether model should predict slope or single weeks\n#Predicting the slope is making the assumption that the decrease is linear\nPREDICT_SLOPE = False\n\n#Gaussian Noise (the reported std error for measurement devices is 70)\n#GAUSS_ALLIGNED is a boolean indicating if the gaussians added to X and y are perfectly correlated or independent\nVALUE_GAUSSIAN_NOISE_ON_FVC = 140\nGAUSSIAN_NOISE_CORRELATED = True\n                                     \n#Activation function to use ('swish' or 'relu')\nACTIVATION_FUNCTION = 'swish'\n\n#Experimenting with loss\nMODIFIED_LOSS = True\n\n#Dropout rate\nDROP_OUT_RATE = 0\nDROP_OUT_LAYERS = [] # [0,1,2] voor dropout in de eerste 3 lagen\n\n#Train length\nEPOCHS = 250\n\n#L2-Regularization\nL2_REGULARIZATION = False\nREGULARIZATION_CONSTANT = 0.0001\n\n#Input and/or output normalization\nINPUT_NORMALIZATION = True\nOUTPUT_NORMALIZATION = True\n\n#Learning rate\nLEARNING_RATE_SCHEDULER = 'exp' #'exp', 'cos' or None\nMAX_LEARNING_RATE = 0.001\nCOSINE_CYCLES = 5\nEPOCHS_PER_OOM_DECAY = 100 #OoM : Order of Magnitude\n\nMODEL_NAME = \"TestExpDecayAdam\" \n\nconfig = dict(NUMBER_FEATURES = NUMBER_FEATURES, L2_REGULARIZATION = L2_REGULARIZATION, INPUT_NORMALIZATION = INPUT_NORMALIZATION,\n              ACTIVATION_FUNCTION = ACTIVATION_FUNCTION, DROP_OUT_RATE = DROP_OUT_RATE, OUTPUT_NORMALIZATION = OUTPUT_NORMALIZATION,\n              EPOCHS = EPOCHS, MAX_LEARNING_RATE = MAX_LEARNING_RATE, MODIFIED_LOSS = MODIFIED_LOSS,\n              COSINE_CYCLES = COSINE_CYCLES, MODEL_NAME=MODEL_NAME, LEARNING_RATE_SCHEDULER = LEARNING_RATE_SCHEDULER,\n              VALUE_GAUSSIAN_NOISE_ON_FVC=VALUE_GAUSSIAN_NOISE_ON_FVC, PREDICT_SLOPE = PREDICT_SLOPE,\n              HIDDEN_LAYERS = HIDDEN_LAYERS, REGULARIZATION_CONSTANT = REGULARIZATION_CONSTANT, EPOCHS_PER_OOM_DECAY = EPOCHS_PER_OOM_DECAY,\n              DROP_OUT_LAYERS = DROP_OUT_LAYERS, BATCH_SIZE = BATCH_SIZE, GAUSSIAN_NOISE_CORRELATED = GAUSSIAN_NOISE_CORRELATED )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    test_data, submission = get_test_data(\"../input/osic-pulmonary-fibrosis-progression/test.csv\", INPUT_NORMALIZATION)\n    \ntrain, data, labels = get_train_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION, TRAIN_ON_BACKWARD_WEEKS)\n\nif PSEUDO_TEST_PATIENTS > 0:\n    test_data, test_check = get_pseudo_test_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = build_model(config)\n#tf.keras.utils.plot_model(model)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Folds and Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_pos = get_fold_indices(FOLDS, train)\nprint(fold_pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DATA_GENERATOR:\n    train_data = train[[\"Weeks\", \"FVC\", \"Percent\", \"Age\", \"Sex\", \n                                 \"Currently smokes\", \"Ex-smoker\", \"Never smoked\", \"Weekdiff_target\"]]\n    train_labels = labels\n    np.save(\"train_data.npy\", train_data.to_numpy())\n    np.save(\"train_labels.npy\", train_labels.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\n\nfor fold in range(FOLDS):\n    if DATA_GENERATOR:\n        train_ID = list(range(fold_pos[0],fold_pos[fold])) + list(range(fold_pos[fold+1],len(train)))\n        val_ID = list(range(fold_pos[fold], fold_pos[fold+1]))\n        # Generators\n        training_generator = DataGenerator(train_ID, config)\n        validation_generator = DataGenerator(val_ID, config, validation = True)\n    else:\n        x_train = data[\"input_features\"][:fold_pos[fold]].append(data[\"input_features\"][fold_pos[fold+1]:])\n        y_train = labels[:fold_pos[fold]].append(labels[fold_pos[fold+1]:])\n        x_val = data[\"input_features\"][fold_pos[fold]:fold_pos[fold+1]]\n        y_val = labels[fold_pos[fold]:fold_pos[fold+1]]\n    \n    model = build_model(config)\n    \n    sv = tf.keras.callbacks.ModelCheckpoint(\n    'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n    save_weights_only=True, mode='min', save_freq='epoch')\n    callbacks = [sv]\n    if LEARNING_RATE_SCHEDULER == 'exp':\n        callbacks.append(get_exponential_decay_lr_callback(config))\n    if LEARNING_RATE_SCHEDULER == 'cos':\n        callbacks.append(get_cosine_annealing_lr_callback(config))\n\n    print(fold+1, \"of\", FOLDS)\n    if WANDB:\n        name = MODEL_NAME + '-F{}'.format(fold+1)\n        config.update({'fold': fold+1})\n        wandb.init(project=\"pulfib\", name=name, config=config)\n        wandb_cb = WandbCallback()\n        callbacks.append(wandb_cb)\n        \n    if DATA_GENERATOR:\n        history = model.fit(training_generator, validation_data = validation_generator, epochs = EPOCHS,\n                            verbose = 0, callbacks = callbacks)\n    else:\n        history = model.fit(x_train, y_train, validation_data = (x_val,y_val), epochs = EPOCHS, verbose = 0, callbacks = callbacks)\n\n    if SUBMIT or PSEUDO_TEST_PATIENTS > 0:\n        model.load_weights('fold-%i.h5'%fold)\n        predictions.append(model.predict(test_data, batch_size = 256))\n    \n    if WANDB:\n        # finalize run\n        wandb.join()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    if PREDICT_SLOPE:\n        predictions = np.mean(predictions,axis = 0)\n        for i in range(1,len(test_data)+1):\n            submission.loc[i,\"FVC\"] = test_data.loc[i-1,\"FVC\"] + predictions[i-1,0]*test_data.loc[i-1,\"Weekdiff_target\"]\n            submission.loc[i, \"Confidence\"] = abs(predictions[i-1,1]*test_data.loc[i-1,\"Weekdiff_target\"])\n    else:\n        predictions = np.abs(predictions)\n        predictions[:,:,1] = np.power(predictions[:,:,1],2)\n        predictions = np.mean(predictions, axis = 0)\n        predictions[:,1] = np.power(predictions[:,1],0.5)\n        for i in range(1,len(test_data)+1):\n            submission.loc[i,\"FVC\"] = predictions[i-1,0]\n            submission.loc[i, \"Confidence\"] = predictions[i-1,1]\n    submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom scipy.stats import gmean\nif PSEUDO_TEST_PATIENTS > 0:\n    result = []\n    for i in range(-20,20):\n        postprocess = np.abs(predictions)\n        if i == 0:\n            postprocess[:,:,1] = gmean(postprocess[:,:,1], axis = 0)\n            postprocess = np.mean(postprocess, axis = 0)\n        else:\n            postprocess[:,:,1] = np.power(postprocess[:,:,1],i)\n            postprocess = np.mean(postprocess, axis = 0)\n            postprocess[:,1] = np.power(postprocess[:,1],1/i)\n        FVC_true = test_check[\"TargetFVC\"].values\n        FVC_pred = postprocess[:,0]\n        sigma = postprocess[:,1]\n\n        sigma_clip = np.maximum(np.abs(sigma), 70)\n        delta = np.abs(FVC_true - FVC_pred)\n        delta = np.minimum(delta, 1000)\n\n        sq2 = np.sqrt(2)\n        loss = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip * sq2)\n        result.append(np.mean(loss))\n    plt.plot(np.arange(-20,20),result)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}