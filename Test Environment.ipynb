{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tqdm import tqdm\nimport seaborn as sns\nimport wandb\nfrom wandb.keras import WandbCallback\nimport keras\nfrom keras.models import Sequential\n\nfrom pfutils import (get_test_data, get_train_data, get_pseudo_test_data, get_exponential_decay_lr_callback,\n                     build_model, get_cosine_annealing_lr_callback, get_fold_indices, DataGenerator)\n\nWANDB = True\nSUBMIT = False\nDATA_GENERATOR = True\nTRAIN_ON_BACKWARD_WEEKS = False\n\n#If TEST is False use this to simulate tractable testcases. Should be 0 if SUBMIT = True\nPSEUDO_TEST_PATIENTS = 0","execution_count":208,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    PSEUDO_TEST_PATIENTS = 0\n    WANDB = False","execution_count":209,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if WANDB:    \n    # retrieve W&B key\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb_key\")\n    assert wandb_key, \"Please create a key.txt or Kaggle Secret with your W&B API key\"\n\n    #wandb_key = \"24020b558f39257d30a084a55cb438922c321495\"\n\n    !pip install wandb -qqq\n    !wandb login $wandb_key","execution_count":210,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Settings And network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of folds. A number between 1 and 176-PSEUDO_TEST_PATIENTS. 176 = 2^4 * 11\nFOLDS = 5\n\n#Batch size\nBATCH_SIZE = 128\n\n#Amount of features inputted in NN\nNUMBER_FEATURES = 9\n\n#Hidden layers\nHIDDEN_LAYERS = [64,64]\n\n#State whether model should predict slope or single weeks\n#Predicting the slope is making the assumption that the decrease is linear\nPREDICT_SLOPE = False\n\n#Gaussian Noise (the reported std error for FVC measurement devices is 70)\n#All values range approximately from 0 to 1 except FVC which is between 0 and 6688\n#0.01 change on Weeks corresponds to 1 week. Week_diff is changed accordingly\n#NOISE_SDS : [Weeks, FVC, Percent, Age, Sex, CurrentlySmokes, Ex-smoker, Never Smoked]\nNOISE_SDS = [0.1, 70, 0.1] + 5*[0.25]\n#GAUSSIAN_NOISE_CORRELATED is a boolean indicating if the gaussians added to X and y are perfectly correlated or independent\nGAUSSIAN_NOISE_CORRELATED = True\n                           \n#Activation function to use ('swish' or 'relu')\nACTIVATION_FUNCTION = 'swish'\n\n#Experimenting with loss\nLOSS_MODIFICATION = 10 #(sqrt2 * delta / 70) * LOSS_MODIFICATION is added to the loss function (a value of 1 gives roughly equal weight to delta and sigma)\nOPTIMAL_SIGMA_LOSS = False\n\n#Dropout rate\nDROP_OUT_RATE = 0\nDROP_OUT_LAYERS = [] # [0,1,2] voor dropout in de eerste 3 lagen\n\n#Train length\nEPOCHS = 250\n\n#L2-Regularization\nL2_REGULARIZATION = False\nREGULARIZATION_CONSTANT = 0.0001\n\n#Input and/or output normalization\nINPUT_NORMALIZATION = True\nOUTPUT_NORMALIZATION = True\n\n#Learning rate\nLEARNING_RATE_SCHEDULER = 'exp' #'exp', 'cos' or None\nMAX_LEARNING_RATE = 0.001\nCOSINE_CYCLES = 5\nEPOCHS_PER_OOM_DECAY = 150 #OoM : Order of Magnitude\n\nMODEL_NAME = \"Baseline\" \n\nconfig = dict(NUMBER_FEATURES = NUMBER_FEATURES, L2_REGULARIZATION = L2_REGULARIZATION, INPUT_NORMALIZATION = INPUT_NORMALIZATION,\n              ACTIVATION_FUNCTION = ACTIVATION_FUNCTION, DROP_OUT_RATE = DROP_OUT_RATE, OUTPUT_NORMALIZATION = OUTPUT_NORMALIZATION,\n              EPOCHS = EPOCHS, MAX_LEARNING_RATE = MAX_LEARNING_RATE, LOSS_MODIFICATION = LOSS_MODIFICATION, NOISE_SDS = NOISE_SDS, OPTIMAL_SIGMA_LOSS = OPTIMAL_SIGMA_LOSS,\n              COSINE_CYCLES = COSINE_CYCLES, MODEL_NAME=MODEL_NAME, LEARNING_RATE_SCHEDULER = LEARNING_RATE_SCHEDULER, PREDICT_SLOPE = PREDICT_SLOPE,\n              HIDDEN_LAYERS = HIDDEN_LAYERS, REGULARIZATION_CONSTANT = REGULARIZATION_CONSTANT, EPOCHS_PER_OOM_DECAY = EPOCHS_PER_OOM_DECAY,\n              DROP_OUT_LAYERS = DROP_OUT_LAYERS, BATCH_SIZE = BATCH_SIZE, GAUSSIAN_NOISE_CORRELATED = GAUSSIAN_NOISE_CORRELATED)","execution_count":211,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    test_data, submission = get_test_data(\"../input/osic-pulmonary-fibrosis-progression/test.csv\", INPUT_NORMALIZATION)\n    \ntrain, data, labels = get_train_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION, TRAIN_ON_BACKWARD_WEEKS)\n\nif PSEUDO_TEST_PATIENTS > 0:\n    test_data, test_check = get_pseudo_test_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION)","execution_count":212,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = build_model(config)\n#tf.keras.utils.plot_model(model)\nmodel.summary()","execution_count":214,"outputs":[{"output_type":"stream","text":"Model: \"functional_179\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_features (InputLayer)     [(None, 9)]          0                                            \n__________________________________________________________________________________________________\ndense_356 (Dense)               (None, 64)           640         input_features[0][0]             \n__________________________________________________________________________________________________\ndense_357 (Dense)               (None, 64)           4160        dense_356[0][0]                  \n__________________________________________________________________________________________________\ndense_358 (Dense)               (None, 64)           4160        dense_357[0][0]                  \n__________________________________________________________________________________________________\nFVC_output (Dense)              (None, 1)            65          dense_358[0][0]                  \n__________________________________________________________________________________________________\nsigma_output (Dense)            (None, 1)            65          dense_358[0][0]                  \n__________________________________________________________________________________________________\ntf_op_layer_Mul_356 (TensorFlow [(None, 1)]          0           FVC_output[0][0]                 \n__________________________________________________________________________________________________\ntf_op_layer_Mul_357 (TensorFlow [(None, 1)]          0           sigma_output[0][0]               \n__________________________________________________________________________________________________\ntf_op_layer_Mul_358 (TensorFlow [(None, 1)]          0           tf_op_layer_Mul_356[0][0]        \n__________________________________________________________________________________________________\ntf_op_layer_Mul_359 (TensorFlow [(None, 1)]          0           tf_op_layer_Mul_357[0][0]        \n__________________________________________________________________________________________________\nslope_FVC (InputLayer)          [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nslope_Weekdiff (InputLayer)     [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nconcatenate_89 (Concatenate)    (None, 2)            0           tf_op_layer_Mul_358[0][0]        \n                                                                 tf_op_layer_Mul_359[0][0]        \n==================================================================================================\nTotal params: 9,090\nTrainable params: 9,090\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Folds and Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_pos = get_fold_indices(FOLDS, train)\nprint(fold_pos)","execution_count":215,"outputs":[{"output_type":"stream","text":"[0, 1225, 2450, 3695, 4847, 6072]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DATA_GENERATOR:\n    train_data = train[[\"Weeks\", \"FVC\", \"Percent\", \"Age\", \"Sex\", \n                                 \"Currently smokes\", \"Ex-smoker\", \"Never smoked\", \"Weekdiff_target\"]]\n    train_labels = labels\n    np.save(\"train_data.npy\", train_data.to_numpy())\n    np.save(\"train_labels.npy\", train_labels.to_numpy())","execution_count":216,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\n\nfor fold in range(FOLDS):\n    if DATA_GENERATOR:\n        train_ID = list(range(fold_pos[0],fold_pos[fold])) + list(range(fold_pos[fold+1],len(train)))\n        val_ID = list(range(fold_pos[fold], fold_pos[fold+1]))\n        #train_ID = list(range(fold_pos[0],fold_pos[fold]))\n        #val_ID = list(range(fold_pos[0],fold_pos[fold]))\n        # Generators\n        training_generator = DataGenerator(train_ID, config)\n        validation_generator = DataGenerator(val_ID, config, validation = True)\n    else:\n        x_train = data[\"input_features\"][:fold_pos[fold]].append(data[\"input_features\"][fold_pos[fold+1]:])\n        y_train = labels[:fold_pos[fold]].append(labels[fold_pos[fold+1]:])\n        x_val = data[\"input_features\"][fold_pos[fold]:fold_pos[fold+1]]\n        y_val = labels[fold_pos[fold]:fold_pos[fold+1]]\n    \n    model = build_model(config)\n    \n    sv = tf.keras.callbacks.ModelCheckpoint(\n    'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n    save_weights_only=True, mode='min', save_freq='epoch')\n    callbacks = [sv]\n    if LEARNING_RATE_SCHEDULER == 'exp':\n        callbacks.append(get_exponential_decay_lr_callback(config))\n    if LEARNING_RATE_SCHEDULER == 'cos':\n        callbacks.append(get_cosine_annealing_lr_callback(config))\n\n    print(fold+1, \"of\", FOLDS)\n    if WANDB:\n        name = MODEL_NAME + '-F{}'.format(fold+1)\n        config.update({'fold': fold+1})\n        wandb.init(project=\"pulfib\", name = name, config=config)\n        wandb_cb = WandbCallback()\n        callbacks.append(wandb_cb)\n        \n    if DATA_GENERATOR:\n        history = model.fit(training_generator, validation_data = validation_generator, epochs = EPOCHS,\n                            verbose = 0, callbacks = callbacks)\n    else:\n        history = model.fit(x_train, y_train, validation_data = (x_val,y_val), epochs = EPOCHS, verbose = 0, callbacks = callbacks)\n\n    if SUBMIT or PSEUDO_TEST_PATIENTS > 0:\n        model.load_weights('fold-%i.h5'%fold)\n        predictions.append(model.predict(test_data, batch_size = 256))\n    \n    if WANDB:\n        # finalize run\n        wandb.join()","execution_count":null,"outputs":[{"output_type":"stream","text":"1 of 5\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/larsranmath/pulfib\" target=\"_blank\">https://app.wandb.ai/larsranmath/pulfib</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/larsranmath/pulfib/runs/ixs0j1hk\" target=\"_blank\">https://app.wandb.ai/larsranmath/pulfib/runs/ixs0j1hk</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"stream","text":"2 of 5\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/larsranmath/pulfib\" target=\"_blank\">https://app.wandb.ai/larsranmath/pulfib</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/larsranmath/pulfib/runs/m21mtvkm\" target=\"_blank\">https://app.wandb.ai/larsranmath/pulfib/runs/m21mtvkm</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"stream","text":"3 of 5\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/larsranmath/pulfib\" target=\"_blank\">https://app.wandb.ai/larsranmath/pulfib</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/larsranmath/pulfib/runs/p2q7geqz\" target=\"_blank\">https://app.wandb.ai/larsranmath/pulfib/runs/p2q7geqz</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    if PREDICT_SLOPE:\n        predictions = np.mean(predictions,axis = 0)\n        for i in range(1,len(test_data)+1):\n            submission.loc[i,\"FVC\"] = test_data.loc[i-1,\"FVC\"] + predictions[i-1,0]*test_data.loc[i-1,\"Weekdiff_target\"]\n            submission.loc[i, \"Confidence\"] = abs(predictions[i-1,1]*test_data.loc[i-1,\"Weekdiff_target\"])\n    else:\n        predictions = np.abs(predictions)\n        predictions[:,:,1] = np.power(predictions[:,:,1],2)\n        predictions = np.mean(predictions, axis = 0)\n        predictions[:,1] = np.power(predictions[:,1],0.5)\n        for i in range(1,len(test_data)+1):\n            submission.loc[i,\"FVC\"] = predictions[i-1,0]\n            submission.loc[i, \"Confidence\"] = predictions[i-1,1]\n    submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom scipy.stats import gmean\nif PSEUDO_TEST_PATIENTS > 0:\n    result = []\n    for i in range(-20,20):\n        postprocess = np.abs(predictions)\n        if i == 0:\n            postprocess[:,:,1] = gmean(postprocess[:,:,1], axis = 0)\n            postprocess = np.mean(postprocess, axis = 0)\n        else:\n            postprocess[:,:,1] = np.power(postprocess[:,:,1],i)\n            postprocess = np.mean(postprocess, axis = 0)\n            postprocess[:,1] = np.power(postprocess[:,1],1/i)\n        FVC_true = test_check[\"TargetFVC\"].values\n        FVC_pred = postprocess[:,0]\n        sigma = postprocess[:,1]\n\n        sigma_clip = np.maximum(np.abs(sigma), 70)\n        delta = np.abs(FVC_true - FVC_pred)\n        delta = np.minimum(delta, 1000)\n\n        sq2 = np.sqrt(2)\n        loss = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip * sq2)\n        result.append(np.mean(loss))\n    plt.plot(np.arange(-20,20),result)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}