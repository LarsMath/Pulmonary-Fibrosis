{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tqdm import tqdm\nimport seaborn as sns\nimport wandb\nfrom wandb.keras import WandbCallback\nimport keras\nfrom keras.models import Sequential\n\nfrom pfutils import (get_test_data, get_train_data, get_pseudo_test_data,\n                     build_model, get_cosine_annealing_lr_callback, get_fold_indices)\n\nWANDB = True\nTEST = False\nDATA_GENERATOR = True\n\n#If TEST is False use this to simulate tractable testcases. Should be 0 if TEST = True\nPSEUDO_TEST_PATIENTS = 0\nif TEST:\n    PSEUDO_TEST_PATIENTS = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wandb_key = \"ea9b3c785541508ffdd795f2a706df065df389e3\"\n\n!pip install -q --upgrade wandb\n!wandb login $wandb_key","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Settings And network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of folds. A number between 1 and 176-PSEUDO_TEST_PATIENTS\nFOLDS = 5\n\n#Batch size\nBATCH_SIZE = 128\n\n#Amount of features inputted in NN\nNUMBER_FEATURES = 9\n\n#Hidden layers\nHIDDEN_LAYERS = [64,32,16,8,4]\n\n#State whether model should predict slope or single weeks\n#Predicting the slope is making the assumption that the decrease is linear\nPREDICT_SLOPE = False\n\n#Gaussian Noise\nUSE_GAUSSIAN_ON_FVC = False \nVALUE_GAUSSIAN_NOISE_ON_FVC = 70 # Only needed when Gaussian noise = True\n                                     \n#Activation function to use ('swish' or 'relu')\nACTIVATION_FUNCTION = 'swish'\n\n#Dropout rate\nDROP_OUT_RATE = 0\nDROP_OUT_LAYERS = [] # [0,1,2] voor dropout in de eerste 3 lagen\n\n#Train length\nEPOCHS = 250\nSTEPS_PER_EPOCH = 100\n\n#L2-Regularization\nL2_REGULARIZATION = False\nREGULARIZATION_CONSTANT = 0.005\n\n#Input and/or output normalization\nINPUT_NORMALIZATION = True\nOUTPUT_NORMALIZATION = True\n\n#Learning rate\nMAX_LEARNING_RATE = 5e-4\nCOSINE_CYCLES = 10\n\nMODEL_NAME = \"Baseline64\"\n\nconfig = dict(NUMBER_FEATURES = NUMBER_FEATURES, L2_REGULARIZATION = L2_REGULARIZATION, INPUT_NORMALIZATION =INPUT_NORMALIZATION,\n              ACTIVATION_FUNCTION = ACTIVATION_FUNCTION, DROP_OUT_RATE = DROP_OUT_RATE, OUTPUT_NORMALIZATION = OUTPUT_NORMALIZATION,\n              EPOCHS = EPOCHS, STEPS_PER_EPOCH = STEPS_PER_EPOCH, MAX_LEARNING_RATE = MAX_LEARNING_RATE,\n              COSINE_CYCLES = COSINE_CYCLES, MODEL_NAME=MODEL_NAME, USE_GAUSSIAN_ON_FVC=USE_GAUSSIAN_ON_FVC,\n              VALUE_GAUSSIAN_NOISE_ON_FVC=VALUE_GAUSSIAN_NOISE_ON_FVC, PREDICT_SLOPE = PREDICT_SLOPE,\n              HIDDEN_LAYERS = HIDDEN_LAYERS, REGULARIZATION_CONSTANT = REGULARIZATION_CONSTANT,\n              DROP_OUT_LAYERS = DROP_OUT_LAYERS, BATCH_SIZE = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TEST:\n    test_data, submission = get_test_data(\"../input/osic-pulmonary-fibrosis-progression/test.csv\")\n    \ntrain, data, labels = get_train_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION)\n\nif PSEUDO_TEST_PATIENTS > 0:\n    test_data, test_check = get_pseudo_test_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ndef roundup(x):\n    return int(math.ceil(x / 10.0)) * 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLDS = 3\n\nnode_size = [256,128,64,32,16,8,4]\n\nfor _ in range(200):\n    HIDDEN_LAYERS = []\n    for i in range(np.random.randint(1,10)):\n        HIDDEN_LAYERS += [node_size[np.random.randint(7)]]\n\n    #Gaussian Noise\n    USE_GAUSSIAN_ON_FVC = np.random.rand() < 0.5\n    VALUE_GAUSSIAN_NOISE_ON_FVC = np.random.randint(200) * USE_GAUSSIAN_ON_FVC # Only needed when Gaussian noise = True\n\n    #Activation function to use ('swish' or 'relu')\n    if np.random.rand() < 0.5:\n        ACTIVATION_FUNCTION = 'swish'\n    else:\n        ACTIVATION_FUNCTION = 'relu'\n\n    rates = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75]    \n    \n    #Dropout rate\n    DROP_OUT_RATE = rates[np.random.randint(1,8)] * (np.random.rand() < 0.3)\n    DROP_OUT_LAYERS = []\n\n    k = np.random.randint(len(HIDDEN_LAYERS))\n    DROP_OUT_LAYERS = list(range(k))\n\n    MODEL_NAME = \"Random HL: \" + str(HIDDEN_LAYERS) + \", GAUSS: \" + str(VALUE_GAUSSIAN_NOISE_ON_FVC) + \", ACTI: \" + ACTIVATION_FUNCTION + \", DROPOUTRATE: \" + str(DROP_OUT_RATE) + \", DROP_OUT_LAYERS: \" + str(DROP_OUT_LAYERS)\n\n    config = dict(NUMBER_FEATURES = NUMBER_FEATURES, L2_REGULARIZATION = L2_REGULARIZATION, INPUT_NORMALIZATION =INPUT_NORMALIZATION,\n                  ACTIVATION_FUNCTION = ACTIVATION_FUNCTION, DROP_OUT_RATE = DROP_OUT_RATE, OUTPUT_NORMALIZATION = OUTPUT_NORMALIZATION,\n                  EPOCHS = EPOCHS, STEPS_PER_EPOCH = STEPS_PER_EPOCH, MAX_LEARNING_RATE = MAX_LEARNING_RATE,\n                  COSINE_CYCLES = COSINE_CYCLES, MODEL_NAME=MODEL_NAME, USE_GAUSSIAN_ON_FVC=USE_GAUSSIAN_ON_FVC,\n                  VALUE_GAUSSIAN_NOISE_ON_FVC=VALUE_GAUSSIAN_NOISE_ON_FVC, PREDICT_SLOPE = PREDICT_SLOPE,\n                  HIDDEN_LAYERS = HIDDEN_LAYERS, LENGTH_HIDDEN_LAYERS = len(HIDDEN_LAYERS),REGULARIZATION_CONSTANT = REGULARIZATION_CONSTANT,\n                  DROP_OUT_LAYERS = DROP_OUT_LAYERS, BATCH_SIZE = BATCH_SIZE, VALUE_GAUSS_IN_BATCH = roundup(VALUE_GAUSSIAN_NOISE_ON_FVC),\n                 FIRST_HIDDEN_LAYERS = roundup(HIDDEN_LAYERS[0]), NUMBER_DROPOUT_LAYERS = len(DROP_OUT_LAYERS))\n\n    lr_cb = get_cosine_annealing_lr_callback(lr_max=config[\"MAX_LEARNING_RATE\"], \n                                            n_epochs=config[\"EPOCHS\"], \n                                            n_cycles=config[\"COSINE_CYCLES\"])\n    \n    fold_pos = get_fold_indices(FOLDS, train)\n    \n    class DataGenerator(keras.utils.Sequence):\n        def __init__(self, list_IDs, config, validation = False, number_of_labels = 3,\n                     batch_size = 128, shuffle = True):\n            self.number_features = int(config[\"NUMBER_FEATURES\"])\n            self.use_gaussian = config[\"USE_GAUSSIAN_ON_FVC\"]\n            self.gauss_std = config[\"VALUE_GAUSSIAN_NOISE_ON_FVC\"] and not validation\n            self.list_IDs = list_IDs\n            self.batch_size = config[\"BATCH_SIZE\"]\n            self.labels = labels\n            self.shuffle = shuffle\n            self.on_epoch_end()\n            self.label_size = number_of_labels\n            self.normalized = config[\"INPUT_NORMALIZATION\"]\n\n        def __len__(self):\n            return int(np.floor(len(self.list_IDs)/self.batch_size))\n\n        def __getitem__(self, index):\n            'Generate one batch of data'\n            # Generate indexes of the batch\n            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n            list_IDs_temp = [self.list_IDs[k] for k in indexes]\n            # Generate data\n            X, y = self.__data_generation(list_IDs_temp)\n            return X, y\n\n        def on_epoch_end(self):\n            'Updates indexes after each epoch'\n            self.indexes = np.arange(len(self.list_IDs))\n            if self.shuffle == True:\n                np.random.shuffle(self.indexes)\n\n        def __data_generation(self, list_IDs_temp):\n            'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n            # Initialization\n            X = np.empty((self.batch_size, self.number_features))\n            y = np.empty((self.batch_size, self.label_size), dtype=int)\n\n            data = np.load(\"./train_data.npy\", allow_pickle = True)\n            lab = np.load(\"./train_labels.npy\", allow_pickle = True)\n\n            gauss = np.asarray(0)\n\n            if self.use_gaussian:\n                gauss = np.random.normal(0, self.gauss_std, size = self.batch_size)\n\n            for i, ID in enumerate(list_IDs_temp):\n                X[i,] = np.asarray(data[ID], dtype = \"float32\")\n                y[i,] = np.asarray(lab[ID], dtype = \"float32\")\n\n            if self.normalized:\n                X[:,1] += gauss.astype(\"float32\")/5800 \n            else:\n                X[:,1] += gauss.astype(\"float32\")\n\n            y = np.asarray(y,dtype = \"float32\")\n            y[:,2] += gauss.astype(\"float32\")\n\n            return X, y\n        \n    if DATA_GENERATOR:\n        train_data = train[[\"Weeks\", \"FVC\", \"Percent\", \"Age\", \"Sex\", \n                                     \"Currently smokes\", \"Ex-smoker\", \"Never smoked\", \"Weekdiff_target\"]]\n        train_labels = labels\n        np.save(\"train_data.npy\", train_data.to_numpy())\n        np.save(\"train_labels.npy\", train_labels.to_numpy())\n        \n    predictions = []\n\n    for fold in range(FOLDS):\n        if DATA_GENERATOR:\n            train_ID = list(range(fold_pos[0],fold_pos[fold])) + list(range(fold_pos[fold+1],len(train)))\n            val_ID = list(range(fold_pos[fold], fold_pos[fold+1]))\n            # Generators\n            training_generator = DataGenerator(train_ID, config)\n            validation_generator = DataGenerator(val_ID, config, validation = True)\n        else:\n            x_train = data[\"input_features\"][:fold_pos[fold]].append(data[\"input_features\"][fold_pos[fold+1]:])\n            y_train = labels[:fold_pos[fold]].append(labels[fold_pos[fold+1]:])\n            x_val = data[\"input_features\"][fold_pos[fold]:fold_pos[fold+1]]\n            y_val = labels[fold_pos[fold]:fold_pos[fold+1]]\n        \n        model = build_model(config)\n        \n        sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min', save_freq='epoch')\n        callbacks = [sv]\n\n        print(fold+1, \"of\", FOLDS)\n        if WANDB:\n            name = MODEL_NAME + '-F{}'.format(fold+1)\n            config.update({'fold': fold+1})\n            wandb.init(project=\"pulfib\", name=name, config=config)\n            wandb_cb = WandbCallback()\n            callbacks.append(wandb_cb)\n            \n        if DATA_GENERATOR:\n            history = model.fit(training_generator, validation_data = validation_generator, epochs = EPOCHS,\n                                verbose = 0, callbacks = callbacks)\n        else:\n            history = model.fit(x_train, y_train, validation_data = (x_val,y_val), epochs = EPOCHS,\n                                steps_per_epoch = STEPS_PER_EPOCH, verbose = 2, callbacks = callbacks)\n\n        if TEST or PSEUDO_TEST_PATIENTS > 0:\n            model.load_weights('fold-%i.h5'%fold)\n            predictions.append(model.predict(test_data, batch_size = 256))\n\n        if WANDB:\n            # finalize run\n            wandb.join()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}