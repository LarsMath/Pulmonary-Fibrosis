{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tqdm import tqdm\nimport seaborn as sns\nimport wandb\nfrom wandb.keras import WandbCallback\nimport keras\nfrom keras.models import Sequential\n\nfrom pfutils import (get_test_data, get_train_data, get_pseudo_test_data,\n                     build_model, get_cosine_annealing_lr_callback, get_fold_indices)\n\nWANDB = True\nSUBMIT = False\nDATA_GENERATOR = True\nTRAIN_ON_BACKWARD_WEEKS = False\n\n#If TEST is False use this to simulate tractable testcases. Should be 0 if SUBMIT = True\nPSEUDO_TEST_PATIENTS = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT:\n    PSEUDO_TEST_PATIENTS = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve W&B key\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb\")\nassert wandb_key, \"Please create a key.txt or Kaggle Secret with your W&B API key\"\n\n#wandb_key = \"24020b558f39257d30a084a55cb438922c321495\"\n\n!pip install -q --upgrade wandb\n!wandb login $wandb_key","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Settings And network"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(config):\n    size = config[\"NUMBER_FEATURES\"]\n    actfunc = config[\"ACTIVATION_FUNCTION\"]\n    predict_slope = config[\"PREDICT_SLOPE\"]\n    drop_out_rate = config[\"DROP_OUT_RATE\"]\n    l2_regularization = config[\"L2_REGULARIZATION\"]\n    output_normalization = config[\"OUTPUT_NORMALIZATION\"]\n    hidden_layers = config[\"HIDDEN_LAYERS\"]\n    regularization_constant = config[\"REGULARIZATION_CONSTANT\"]\n    drop_out_layers = config[\"DROP_OUT_LAYERS\"]\n    Learn_on = config[\"LEARN_ON\"]\n    \n    if actfunc == 'swish':\n        actfunc = tf.keras.activations.swish\n\n    inp = tf.keras.layers.Input(shape=(size), name = \"input_features\")\n    inp2 = tf.keras.layers.Input(shape=(1), name = \"slope_FVC\")\n    inp3 = tf.keras.layers.Input(shape=(1), name = \"slope_Weekdiff\")\n    \n    inputs = [inp,inp2,inp3]\n\n    x = inp\n    \n    for j,n_neurons in enumerate(hidden_layers):\n        if l2_regularization:\n            x = tf.keras.layers.Dense(n_neurons, activation=actfunc,\n                                      kernel_regularizer = tf.keras.regularizers.l2(regularization_constant))(x)\n        else:\n            x = tf.keras.layers.Dense(n_neurons, activation=actfunc)(x)\n        if j in drop_out_layers:\n            x = tf.keras.layers.Dropout(drop_out_rate)(x)\n    \n    FVC_output = tf.keras.layers.Dense(1, name = \"FVC_output\")(x)\n    sigma_output = tf.keras.layers.Dense(1, name = \"sigma_output\")(x)\n    \n    if output_normalization:\n        FVC_output = tf.math.scalar_mul(tf.constant(50,dtype = 'float32'), FVC_output)\n        sigma_output = tf.math.scalar_mul(tf.constant(5,dtype = 'float32'), sigma_output)\n        if(not predict_slope):\n            FVC_output = tf.math.scalar_mul(tf.constant(100,dtype = 'float32'), FVC_output)\n            sigma_output = tf.math.scalar_mul(tf.constant(100,dtype = 'float32'), sigma_output)\n\n    if False:\n        FVC_output = tf.add(tf.keras.layers.multiply([FVC_output, inp2]),inp3)\n        sigma_output = tf.keras.layers.multiply([sigma_output, inp2])\n        \n    outputs = tf.keras.layers.concatenate([FVC_output,sigma_output])\n    \n    def absolute_delta_error(y_true, y_pred):\n        y_true = tf.dtypes.cast(y_true, tf.float32)\n        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n        FVC_true = y_true[:,0]\n        \n        if(predict_slope):\n            slope = y_pred[:,0]\n            s = y_pred[:,1]\n\n            weeks_from_start = y_true[:,1]\n            FVC_start = y_true[:,2]\n            \n            sigma = s * weeks_from_start\n            # Kan probleem worden by ReLu omdat slope negatief wordt door minimalisering Loss\n            FVC_pred = weeks_from_start * slope + FVC_start\n        else:\n            FVC_pred = tf.abs(y_pred[:,0])\n        \n        ## ** Hier kan een fout komen doordat de afgeleide moeilijker te berekenen is\n        delta = tf.abs(FVC_true - FVC_pred)\n        ## **\n    \n        loss = delta\n        return K.mean(loss)\n\n\n    def sigma_cost(y_true, y_pred):\n        y_true = tf.dtypes.cast(y_true, tf.float32)\n        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n        \n        if(predict_slope):\n            slope = y_pred[:,0]\n            s = y_pred[:,1]\n\n            weeks_from_start = y_true[:,1]\n            FVC_start = y_true[:,2]\n            \n            sigma = s * weeks_from_start\n            # Kan probleem worden by ReLu omdat slope negatief wordt door minimalisering Loss\n            FVC_pred = weeks_from_start * slope + FVC_start\n        else:\n            sigma = y_pred[:,1]\n        \n        sigma_clip = tf.maximum(tf.abs(sigma), 70)\n        \n        sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))\n        loss = tf.math.log(sigma_clip * sq2)\n        return K.mean(loss)\n    \n    def delta_over_sigma(y_true, y_pred):\n        y_true = tf.dtypes.cast(y_true, tf.float32)\n        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n        FVC_true = y_true[:,0]\n        \n        if(predict_slope):\n            slope = y_pred[:,0]\n            s = y_pred[:,1]\n\n            weeks_from_start = y_true[:,1]\n            FVC_start = y_true[:,2]\n            \n            sigma = s * weeks_from_start\n            # Kan probleem worden by ReLu omdat slope negatief wordt door minimalisering Loss\n            FVC_pred = weeks_from_start * slope + FVC_start\n        else:\n            FVC_pred = tf.abs(y_pred[:,0])\n            sigma = tf.abs(y_pred[:,1])\n        \n        ## ** Hier kan een fout komen doordat de afgeleide moeilijker te berekenen is\n        sigma_clip = tf.maximum(tf.abs(sigma), 70)\n        delta = tf.abs(FVC_true - FVC_pred)\n        delta = tf.minimum(delta, 1000)\n        ## **\n        \n        sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))\n        loss = (delta / sigma_clip)*sq2\n        return K.mean(loss)\n    \n    def Laplace_log_likelihood(y_true, y_pred):\n        y_true = tf.dtypes.cast(y_true, tf.float32)\n        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n        FVC_true = y_true[:,0]\n        \n        if predict_slope:\n            slope = y_pred[:,0]\n            s = y_pred[:,1]\n\n            weeks_from_start = y_true[:,1]\n            FVC_start = y_true[:,2]\n            \n            sigma = s * weeks_from_start\n            # Kan probleem worden by ReLu omdat slope negatief wordt door minimalisering Loss\n            FVC_pred = weeks_from_start * slope + FVC_start\n        else:\n            FVC_pred = tf.abs(y_pred[:,0])\n            sigma = tf.abs(y_pred[:,1])\n        \n        ## ** Hier kan een fout komen doordat de afgeleide moeilijker te berekenen is\n        sigma = tf.maximum(tf.abs(sigma), 70)\n        delta = tf.abs(FVC_true - FVC_pred)\n        ## **\n        \n        sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))\n        loss = (delta / sigma)*sq2 + tf.math.log(sigma * sq2)\n        return K.mean(loss)\n    \n    def Laplace_metric(y_true, y_pred):\n        y_true = tf.dtypes.cast(y_true, tf.float32)\n        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n        FVC_true = y_true[:,0]\n        \n        if predict_slope:\n            slope = y_pred[:,0]\n            s = y_pred[:,1]\n\n            weeks_from_start = y_true[:,1]\n            FVC_start = y_true[:,2]\n            \n            sigma = s * weeks_from_start\n            # Kan probleem worden by ReLu omdat slope negatief wordt door minimalisering Loss\n            FVC_pred = weeks_from_start * slope + FVC_start\n        else:\n            FVC_pred = tf.abs(y_pred[:,0])\n            sigma = tf.abs(y_pred[:,1])\n        \n        ## ** Hier kan een fout komen doordat de afgeleide moeilijker te berekenen is\n        sigma_clip = tf.maximum(tf.abs(sigma), 70)\n        delta = tf.abs(FVC_true - FVC_pred)\n        delta = tf.minimum(delta, 1000)\n        ## **\n        \n        sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))\n        loss = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip * sq2)\n        return K.mean(loss)\n    \n    if Learn_on:\n        opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n    else: \n        opt = tf.keras.optimizers.Adam()\n    model = tf.keras.Model(inputs = inputs, outputs = outputs)    \n    \n    model.compile(optimizer=opt, loss=Laplace_log_likelihood,\n                  metrics = [Laplace_metric, sigma_cost, delta_over_sigma, absolute_delta_error])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\n\n# Number of folds. A number between 1 and 176-PSEUDO_TEST_PATIENTS\nFOLDS = 3\n\n#Batch size\nBATCH_SIZE = 128\n\n#Dropout rate\nDROP_OUT_RATE = 0\nDROP_OUT_LAYERS = [] # [0,1,2] voor dropout in de eerste 3 lagen\n\n#Train length\nEPOCHS = 250\n\n#L2-Regularization\nL2_REGULARIZATION = False\nREGULARIZATION_CONSTANT = 0.005\n\n#Input and/or output normalization\nINPUT_NORMALIZATION = True\nOUTPUT_NORMALIZATION = True\nCOSINE_CYCLES = 5\nMAX_LEARNING_RATE = 5e-4\n#Amount of features inputted in NN\nNUMBER_FEATURES = 9\nUSE_GAUSSIAN_ON_FVC = True \nACTIVATION_FUNCTION = 'swish'\n\nTrial_Layers = [[256], [128], [64], [256,128], [128,64], [64,64], [128,64,64]]\nTrial_Gauss = [0,70,120,180]\nTrial_Correlating_Gauss = [0,1]\nTrial_Forwards = [0,1]\nLearning_rate_automatic = [0,1]\n\nSTEPS_PER_EPOCH = 100\nPREDICT_SLOPE = 0\n\nfor qqq in range(200):\n    HIDDEN_LAYERS = Trial_Layers[np.random.randint(7)]\n    VALUE_GAUSSIAN_NOISE_ON_FVC = Trial_Gauss[np.random.randint(4)]\n    GAUSSIAN_NOISE_CORRELATED = np.random.rand(1) > 0.5\n    LEARN_ON = np.random.rand(1) > 0.5\n    FORWARD_ONLY = np.random.rand(1) > 0.5\n\n    MODEL_NAME = \"Random: \" + str(qqq)\n\n    config = dict(NUMBER_FEATURES = NUMBER_FEATURES, L2_REGULARIZATION = L2_REGULARIZATION, INPUT_NORMALIZATION = INPUT_NORMALIZATION,\n              ACTIVATION_FUNCTION = ACTIVATION_FUNCTION, DROP_OUT_RATE = DROP_OUT_RATE, OUTPUT_NORMALIZATION = OUTPUT_NORMALIZATION,\n              EPOCHS = EPOCHS, STEPS_PER_EPOCH = STEPS_PER_EPOCH, MAX_LEARNING_RATE = MAX_LEARNING_RATE,\n              COSINE_CYCLES = COSINE_CYCLES, MODEL_NAME=MODEL_NAME, USE_GAUSSIAN_ON_FVC=USE_GAUSSIAN_ON_FVC,\n              VALUE_GAUSSIAN_NOISE_ON_FVC=VALUE_GAUSSIAN_NOISE_ON_FVC, PREDICT_SLOPE = PREDICT_SLOPE,\n              HIDDEN_LAYERS = HIDDEN_LAYERS, REGULARIZATION_CONSTANT = REGULARIZATION_CONSTANT,\n              DROP_OUT_LAYERS = DROP_OUT_LAYERS, BATCH_SIZE = BATCH_SIZE, GAUSSIAN_NOISE_CORRELATED = GAUSSIAN_NOISE_CORRELATED, \n              LEARN_ON = LEARN_ON, FORWARD_ONLY = FORWARD_ONLY)\n    \n    train, data, labels = get_train_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION, TRAIN_ON_BACKWARD_WEEKS)\n    \n    fold_pos = get_fold_indices(FOLDS, train)\n    \n    class DataGenerator(keras.utils.Sequence):\n        def __init__(self, list_IDs, config, validation = False, number_of_labels = 3,\n                     batch_size = 128, shuffle = True):\n            self.number_features = int(config[\"NUMBER_FEATURES\"])\n            self.validation = validation\n            self.use_gaussian = config[\"USE_GAUSSIAN_ON_FVC\"]\n            self.gauss_std = config[\"VALUE_GAUSSIAN_NOISE_ON_FVC\"] and not validation\n            self.list_IDs = list_IDs\n            self.batch_size = config[\"BATCH_SIZE\"]\n            self.labels = labels\n            self.shuffle = shuffle\n            self.on_epoch_end()\n            self.label_size = number_of_labels\n            self.normalized = config[\"INPUT_NORMALIZATION\"]\n            self.correlated = config[\"GAUSSIAN_NOISE_CORRELATED\"]\n\n        def __len__(self):\n            return int(np.floor(len(self.list_IDs)/self.batch_size))\n\n        def __getitem__(self, index):\n            'Generate one batch of data'\n            # Generate indexes of the batch\n            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n            list_IDs_temp = [self.list_IDs[k] for k in indexes]\n            # Generate data\n            X, y = self.__data_generation(list_IDs_temp)\n            return X, y\n\n        def on_epoch_end(self):\n            'Updates indexes after each epoch'\n            self.indexes = np.arange(len(self.list_IDs))\n            if self.shuffle == True:\n                np.random.shuffle(self.indexes)\n\n        def __data_generation(self, list_IDs_temp):\n            'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n            # Initialization\n            X = np.empty((self.batch_size, self.number_features))\n            y = np.empty((self.batch_size, self.label_size), dtype=int)\n\n            data = np.load(\"./train_data.npy\", allow_pickle = True)\n            lab = np.load(\"./train_labels.npy\", allow_pickle = True)\n\n            for i, ID in enumerate(list_IDs_temp):\n                X[i,] = np.asarray(data[ID], dtype = \"float32\")\n                y[i,] = np.asarray(lab[ID], dtype = \"float32\")\n            y = np.asarray(y,dtype = \"float32\")\n\n            gauss_X = np.random.normal(0, self.gauss_std, size = self.batch_size)\n\n            if self.correlated:\n                gauss_y = gauss_X\n            else:\n                gauss_y = np.random.normal(0, self.gauss_std, size = self.batch_size)\n            if self.normalized:\n                gauss_X = gauss_X/5000 \n\n            if self.validation: \n                gauss_X = np.random.normal(0, 0, size = self.batch_size)\n                gauss_y = np.random.normal(0, 0, size = self.batch_size)\n            \n            X[:,2] += gauss_X.astype(\"float32\")*X[:,2]/X[:,1]\n            X[:,1] += gauss_X.astype(\"float32\")\n            y[:,2] += gauss_X.astype(\"float32\")\n            y[:,0] += gauss_y.astype(\"float32\")\n\n            return X, y\n    \n    if DATA_GENERATOR:\n        train_data = train[[\"Weeks\", \"FVC\", \"Percent\", \"Age\", \"Sex\", \n                                     \"Currently smokes\", \"Ex-smoker\", \"Never smoked\", \"Weekdiff_target\"]]\n        train_labels = labels\n        np.save(\"train_data.npy\", train_data.to_numpy())\n        np.save(\"train_labels.npy\", train_labels.to_numpy())\n    \n    for fold in range(FOLDS):\n        if DATA_GENERATOR:\n            train_ID = list(range(fold_pos[0],fold_pos[fold])) + list(range(fold_pos[fold+1],len(train)))\n            val_ID = list(range(fold_pos[fold], fold_pos[fold+1]))\n            # Generators\n            training_generator = DataGenerator(train_ID, config)\n            validation_generator = DataGenerator(val_ID, config, validation = True)\n        else:\n            x_train = data[\"input_features\"][:fold_pos[fold]].append(data[\"input_features\"][fold_pos[fold+1]:])\n            y_train = labels[:fold_pos[fold]].append(labels[fold_pos[fold+1]:])\n            x_val = data[\"input_features\"][fold_pos[fold]:fold_pos[fold+1]]\n            y_val = labels[fold_pos[fold]:fold_pos[fold+1]]\n\n        model = build_model(config)\n\n        sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min', save_freq='epoch')\n        callbacks = [sv]\n\n        print(fold+1, \"of\", FOLDS)\n        if WANDB:\n            name = MODEL_NAME + '-F{}'.format(fold+1)\n            config.update({'fold': fold+1})\n            wandb.init(project=\"pulfibrandom2\", name=name, config=config)\n            wandb_cb = WandbCallback()\n            callbacks.append(wandb_cb)\n\n        if DATA_GENERATOR:\n            history = model.fit(training_generator, validation_data = validation_generator, epochs = EPOCHS,\n                                verbose = 0, callbacks = callbacks)\n        else:\n            history = model.fit(x_train, y_train, validation_data = (x_val,y_val), epochs = EPOCHS,\n                                steps_per_epoch = STEPS_PER_EPOCH, verbose = 0, callbacks = callbacks)\n\n        if SUBMIT or PSEUDO_TEST_PATIENTS > 0:\n            model.load_weights('fold-%i.h5'%fold)\n            predictions.append(model.predict(test_data, batch_size = 256))\n\n        if WANDB:\n            # finalize run\n            wandb.join()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}