{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "from pfutils import (get_test_data, get_train_data, get_pseudo_test_data, get_exponential_decay_lr_callback,\n",
    "                     build_model, get_cosine_annealing_lr_callback, get_fold_indices, DataGenerator)\n",
    "\n",
    "WANDB = True\n",
    "SUBMIT = False\n",
    "DATA_GENERATOR = True\n",
    "TRAIN_ON_BACKWARD_WEEKS = False\n",
    "\n",
    "#If TEST is False use this to simulate tractable testcases. Should be 0 if SUBMIT = True\n",
    "PSEUDO_TEST_PATIENTS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMIT:\n",
    "    PSEUDO_TEST_PATIENTS = 0\n",
    "    WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:    \n",
    "    # retrieve W&B key\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "    assert wandb_key, \"Please create a key.txt or Kaggle Secret with your W&B API key\"\n",
    "\n",
    "    #wandb_key = \"24020b558f39257d30a084a55cb438922c321495\"\n",
    "\n",
    "    !pip install -q --upgrade wandb\n",
    "    !wandb login $wandb_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings And network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds. A number between 1 and 176-PSEUDO_TEST_PATIENTS\n",
    "FOLDS = 10\n",
    "\n",
    "#Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#Amount of features inputted in NN\n",
    "NUMBER_FEATURES = 9\n",
    "\n",
    "#Hidden layers\n",
    "HIDDEN_LAYERS = [64,64]\n",
    "\n",
    "#State whether model should predict slope or single weeks\n",
    "#Predicting the slope is making the assumption that the decrease is linear\n",
    "PREDICT_SLOPE = False\n",
    "\n",
    "#Gaussian Noise (the reported std error for FVC measurement devices is 70)\n",
    "#All values range approximately from 0 to 1 except FVC which is between 0 and 6688\n",
    "#0.01 change on Weeks corresponds to 1 week. Week_diff is changed accordingly\n",
    "#NOISE_SDS : [Weeks, FVC, Percent, Age, Sex, CurrentlySmokes, Ex-smoker, Never Smoked]\n",
    "NOISE_SDS = [0.05, 70, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "#GAUSSIAN_NOISE_CORRELATED is a boolean indicating if the gaussians added to X and y are perfectly correlated or independent\n",
    "GAUSSIAN_NOISE_CORRELATED = True\n",
    "                                     \n",
    "#Activation function to use ('swish' or 'relu')\n",
    "ACTIVATION_FUNCTION = 'swish'\n",
    "\n",
    "#Experimenting with loss\n",
    "MODIFIED_LOSS = True\n",
    "\n",
    "#Dropout rate\n",
    "DROP_OUT_RATE = 0\n",
    "DROP_OUT_LAYERS = [] # [0,1,2] voor dropout in de eerste 3 lagen\n",
    "\n",
    "#Train length\n",
    "EPOCHS = 250\n",
    "\n",
    "#L2-Regularization\n",
    "L2_REGULARIZATION = False\n",
    "REGULARIZATION_CONSTANT = 0.0001\n",
    "\n",
    "#Input and/or output normalization\n",
    "INPUT_NORMALIZATION = True\n",
    "OUTPUT_NORMALIZATION = True\n",
    "\n",
    "#Learning rate\n",
    "LEARNING_RATE_SCHEDULER = 'exp' #'exp', 'cos' or None\n",
    "MAX_LEARNING_RATE = 0.001\n",
    "COSINE_CYCLES = 5\n",
    "EPOCHS_PER_OOM_DECAY = 100 #OoM : Order of Magnitude\n",
    "\n",
    "MODEL_NAME = \"TestExpDecayAdam\" \n",
    "\n",
    "config = dict(NUMBER_FEATURES = NUMBER_FEATURES, L2_REGULARIZATION = L2_REGULARIZATION, INPUT_NORMALIZATION = INPUT_NORMALIZATION,\n",
    "              ACTIVATION_FUNCTION = ACTIVATION_FUNCTION, DROP_OUT_RATE = DROP_OUT_RATE, OUTPUT_NORMALIZATION = OUTPUT_NORMALIZATION,\n",
    "              EPOCHS = EPOCHS, MAX_LEARNING_RATE = MAX_LEARNING_RATE, MODIFIED_LOSS = MODIFIED_LOSS, NOISE_SDS = NOISE_SDS,\n",
    "              COSINE_CYCLES = COSINE_CYCLES, MODEL_NAME=MODEL_NAME, LEARNING_RATE_SCHEDULER = LEARNING_RATE_SCHEDULER, PREDICT_SLOPE = PREDICT_SLOPE,\n",
    "              HIDDEN_LAYERS = HIDDEN_LAYERS, REGULARIZATION_CONSTANT = REGULARIZATION_CONSTANT, EPOCHS_PER_OOM_DECAY = EPOCHS_PER_OOM_DECAY,\n",
    "              DROP_OUT_LAYERS = DROP_OUT_LAYERS, BATCH_SIZE = BATCH_SIZE, GAUSSIAN_NOISE_CORRELATED = GAUSSIAN_NOISE_CORRELATED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMIT:\n",
    "    test_data, submission = get_test_data(\"../input/osic-pulmonary-fibrosis-progression/test.csv\", INPUT_NORMALIZATION)\n",
    "    \n",
    "train, data, labels = get_train_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION, TRAIN_ON_BACKWARD_WEEKS)\n",
    "\n",
    "if PSEUDO_TEST_PATIENTS > 0:\n",
    "    test_data, test_check = get_pseudo_test_data('../input/osic-pulmonary-fibrosis-progression/train.csv', PSEUDO_TEST_PATIENTS, INPUT_NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model(config)\n",
    "#tf.keras.utils.plot_model(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_pos = get_fold_indices(FOLDS, train)\n",
    "print(fold_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_GENERATOR:\n",
    "    train_data = train[[\"Weeks\", \"FVC\", \"Percent\", \"Age\", \"Sex\", \n",
    "                                 \"Currently smokes\", \"Ex-smoker\", \"Never smoked\", \"Weekdiff_target\"]]\n",
    "    train_labels = labels\n",
    "    np.save(\"train_data.npy\", train_data.to_numpy())\n",
    "    np.save(\"train_labels.npy\", train_labels.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    if DATA_GENERATOR:\n",
    "        train_ID = list(range(fold_pos[0],fold_pos[fold])) + list(range(fold_pos[fold+1],len(train)))\n",
    "        val_ID = list(range(fold_pos[fold], fold_pos[fold+1]))\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(train_ID, config)\n",
    "        validation_generator = DataGenerator(val_ID, config, validation = True)\n",
    "    else:\n",
    "        x_train = data[\"input_features\"][:fold_pos[fold]].append(data[\"input_features\"][fold_pos[fold+1]:])\n",
    "        y_train = labels[:fold_pos[fold]].append(labels[fold_pos[fold+1]:])\n",
    "        x_val = data[\"input_features\"][fold_pos[fold]:fold_pos[fold+1]]\n",
    "        y_val = labels[fold_pos[fold]:fold_pos[fold+1]]\n",
    "    \n",
    "    model = build_model(config)\n",
    "    \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=True, mode='min', save_freq='epoch')\n",
    "    callbacks = [sv]\n",
    "    if LEARNING_RATE_SCHEDULER == 'exp':\n",
    "        callbacks.append(get_exponential_decay_lr_callback(config))\n",
    "    if LEARNING_RATE_SCHEDULER == 'cos':\n",
    "        callbacks.append(get_cosine_annealing_lr_callback(config))\n",
    "\n",
    "    print(fold+1, \"of\", FOLDS)\n",
    "    if WANDB:\n",
    "        name = MODEL_NAME + '-F{}'.format(fold+1)\n",
    "        config.update({'fold': fold+1})\n",
    "        wandb.init(project=\"pulfib\", name=name, config=config)\n",
    "        wandb_cb = WandbCallback()\n",
    "        callbacks.append(wandb_cb)\n",
    "        \n",
    "    if DATA_GENERATOR:\n",
    "        history = model.fit(training_generator, validation_data = validation_generator, epochs = EPOCHS,\n",
    "                            verbose = 0, callbacks = callbacks)\n",
    "    else:\n",
    "        history = model.fit(x_train, y_train, validation_data = (x_val,y_val), epochs = EPOCHS, verbose = 0, callbacks = callbacks)\n",
    "\n",
    "    if SUBMIT or PSEUDO_TEST_PATIENTS > 0:\n",
    "        model.load_weights('fold-%i.h5'%fold)\n",
    "        predictions.append(model.predict(test_data, batch_size = 256))\n",
    "    \n",
    "    if WANDB:\n",
    "        # finalize run\n",
    "        wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMIT:\n",
    "    if PREDICT_SLOPE:\n",
    "        predictions = np.mean(predictions,axis = 0)\n",
    "        for i in range(1,len(test_data)+1):\n",
    "            submission.loc[i,\"FVC\"] = test_data.loc[i-1,\"FVC\"] + predictions[i-1,0]*test_data.loc[i-1,\"Weekdiff_target\"]\n",
    "            submission.loc[i, \"Confidence\"] = abs(predictions[i-1,1]*test_data.loc[i-1,\"Weekdiff_target\"])\n",
    "    else:\n",
    "        predictions = np.abs(predictions)\n",
    "        predictions[:,:,1] = np.power(predictions[:,:,1],2)\n",
    "        predictions = np.mean(predictions, axis = 0)\n",
    "        predictions[:,1] = np.power(predictions[:,1],0.5)\n",
    "        for i in range(1,len(test_data)+1):\n",
    "            submission.loc[i,\"FVC\"] = predictions[i-1,0]\n",
    "            submission.loc[i, \"Confidence\"] = predictions[i-1,1]\n",
    "    submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gmean\n",
    "if PSEUDO_TEST_PATIENTS > 0:\n",
    "    result = []\n",
    "    for i in range(-20,20):\n",
    "        postprocess = np.abs(predictions)\n",
    "        if i == 0:\n",
    "            postprocess[:,:,1] = gmean(postprocess[:,:,1], axis = 0)\n",
    "            postprocess = np.mean(postprocess, axis = 0)\n",
    "        else:\n",
    "            postprocess[:,:,1] = np.power(postprocess[:,:,1],i)\n",
    "            postprocess = np.mean(postprocess, axis = 0)\n",
    "            postprocess[:,1] = np.power(postprocess[:,1],1/i)\n",
    "        FVC_true = test_check[\"TargetFVC\"].values\n",
    "        FVC_pred = postprocess[:,0]\n",
    "        sigma = postprocess[:,1]\n",
    "\n",
    "        sigma_clip = np.maximum(np.abs(sigma), 70)\n",
    "        delta = np.abs(FVC_true - FVC_pred)\n",
    "        delta = np.minimum(delta, 1000)\n",
    "\n",
    "        sq2 = np.sqrt(2)\n",
    "        loss = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip * sq2)\n",
    "        result.append(np.mean(loss))\n",
    "    plt.plot(np.arange(-20,20),result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
